{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uday160386/ai_ml_exercises/blob/main/Group_14_Voice_E_commerce_Ordering_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIubkln0AI2"
      },
      "source": [
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alui-GbzEBtW"
      },
      "source": [
        "#@title Explanation Video\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<video width=\"854\" and height=\"480\" controls>\n",
        "  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Hackathon_Voice_based.mp4\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LNbxek40AI4"
      },
      "source": [
        "# Hackathon: Voice commands based E-commerce ordering system\n",
        "The goal of the hackathon is to train your model on different types of voice data (such as studio data and your own team data) and able to place order based on user preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fms7Yt7byCuQ"
      },
      "source": [
        "## Grading = 40 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUtVl7cBHlIh"
      },
      "source": [
        "### **Objectives:**\n",
        "\n",
        "Stage 0 - Obtain Features from Audio samples\n",
        "\n",
        "Stage 1 (22 Marks) - Define and train a CNN model on Studio data and deploy the model in the server\n",
        "\n",
        "Stage 2 (18 Marks) - Collect your voice samples (team data) and refine the classifier trained on Studio_data. Deploy the model in the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYm_60PiPSsq"
      },
      "source": [
        "## Dataset Description"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PUI_8X8VCkrr",
        "outputId": "001e6ea9-99bf-437e-891a-e7010af018ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAu1XJx3lCJ"
      },
      "source": [
        "The data contains voice samples of classes - Zero, One, Two, Three, Four, Five. Each class is denoted by a numerical label from 0 to 5.\n",
        "\n",
        "The audio files collected in a Studio dataset contain very few noise samples and all the files are in wav format.\n",
        "\n",
        "The audio files recorded for the studio are saved with the following naming convention:\n",
        "\n",
        "● Class Representation + user_id + sample_ID (or noise + sample_ID)\n",
        "\n",
        "> For example: The voice sample by the user b2 recorded “Zero”, it is saved as 0_b2_35.wav. Here 35 is sample ID, 2 is the user id and ‘0’ is the label of that sample.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv0xxq_d0Qb_",
        "outputId": "42b67cd5-c0a3-4577-9783-51fe32d137a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Please run the setup to download the dataset\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"Hackathon2 - Voice E-commerce Ordering System\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Hackathon_data/B17_studio_rev_data.zip\")\n",
        "    ipython.magic(\"sx unzip B17_studio_rev_data.zip \")\n",
        "    print (\"Setup completed successfully\")\n",
        "\n",
        "setup()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqNBNvC25WNV"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import torch\n",
        "import librosa\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from time import sleep\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEg2PYXrOjnZ"
      },
      "source": [
        "## **Stage 0:** Obtain Features from Audio samples\n",
        "---\n",
        "\n",
        "### Generate features from an audio sample of '.wav' format\n",
        "- Code is available to extract the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTtb2zAj5k0-"
      },
      "source": [
        "# Caution: Do not change the default parameters\n",
        "def get_features(filepath, sr=8000, n_mfcc=30, n_mels=128, frames = 15):\n",
        "    # The following function contains code to produce features of the audio sample.\n",
        "    y, sr = librosa.load(filepath, sr=sr)\n",
        "    D = np.abs(librosa.stft(y))**2\n",
        "    S = librosa.feature.melspectrogram(S=D)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    log_S = librosa.power_to_db(S,ref=np.max)\n",
        "    features = librosa.feature.mfcc(S=log_S, n_mfcc=n_mfcc)\n",
        "    if features.shape[1] < frames :\n",
        "        features = np.hstack((features, np.zeros((n_mfcc, frames - features.shape[1]))))\n",
        "    elif features.shape[1] > frames:\n",
        "        features = features[:, :frames]\n",
        "\n",
        "    # Find 1st order delta_mfcc\n",
        "    delta1_mfcc = librosa.feature.delta(features, order=1)\n",
        "\n",
        "    # Find 2nd order delta_mfcc\n",
        "    delta2_mfcc = librosa.feature.delta(features, order=2)\n",
        "\n",
        "    # Stacking delta_mfcc features in sequence horizontally (column wise)\n",
        "    features = np.hstack((delta1_mfcc.flatten(), delta2_mfcc.flatten()))\n",
        "\n",
        "    # Increase the dimension by inserting an axis along second dimension\n",
        "    features = features.flatten()[:,np.newaxis]\n",
        "\n",
        "    # Convert the numpy.ndarray to a Tensor object\n",
        "    features = Variable(torch.from_numpy(features)).float()\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhLFY4n6BwIj"
      },
      "source": [
        "All the voice samples needed for training are present in the folder `\"studio_data\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMF1AqHZhl1h",
        "outputId": "d6ac5917-fb8c-473c-d1f3-e1d1ae2dcf39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B17_studio_rev_data.zip  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mstudio_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2AAbFp5KORl"
      },
      "source": [
        "##**Stage 1**:  Define and train a CNN model on Studio data and deploy the model in the server\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB-LowDuCMUL"
      },
      "source": [
        "### a) Extract features of Studio data (4 Marks)\n",
        "\n",
        " Load 'Studio data' and extract mfcc features\n",
        "\n",
        " **Evaluation Criteria:**\n",
        "\n",
        " * Complete the code in the load_data function\n",
        " * The function should take path of the folder containing audio samples as input\n",
        " * It should return features of all the audio samples present in the specified folder into single array (list of lists or 2-d numpy array) and their respective labels should be returned too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDzCa-532EUj"
      },
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def load_data(folder_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through each audio file in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Check if the file is a WAV file\n",
        "        if filename.endswith('.wav'):\n",
        "            # Extract class label from the filename\n",
        "            match = re.match(r'(\\d+)_.*', filename)\n",
        "            if match:\n",
        "                label = int(match.group(1))\n",
        "            else:\n",
        "                continue  # Skip files that do not match the naming convention\n",
        "\n",
        "            # Extract features from the audio file using the get_features function\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            file_features = get_features(file_path)\n",
        "\n",
        "            # Append the features to the list of features\n",
        "            features.append(file_features)\n",
        "\n",
        "            # Append the label to the list of labels\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert the lists of features and labels to numpy arrays\n",
        "    features = np.array(features)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7673ezpxFEfM"
      },
      "source": [
        "Load data from studio_data folder for extracting all features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5CjrlPVPjNs"
      },
      "source": [
        "studio_recorded_features, studio_recorded_labels = load_data('studio_data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the loaded features and labels\n",
        "print(\"Loaded Features:\")\n",
        "print(studio_recorded_features)\n",
        "print(\"\\nLoaded Labels:\")\n",
        "print(studio_recorded_labels)"
      ],
      "metadata": {
        "id": "A5ZZPXLlHLAK",
        "outputId": "d84a0a9f-c104-469a-bbd2-f1a84d922ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Features:\n",
            "[[[-8.801408  ]\n",
            "  [-8.801408  ]\n",
            "  [-8.801408  ]\n",
            "  ...\n",
            "  [ 0.1069589 ]\n",
            "  [ 0.1069589 ]\n",
            "  [ 0.1069589 ]]\n",
            "\n",
            " [[10.256201  ]\n",
            "  [10.256201  ]\n",
            "  [10.256201  ]\n",
            "  ...\n",
            "  [-0.9184527 ]\n",
            "  [-0.9184527 ]\n",
            "  [-0.9184527 ]]\n",
            "\n",
            " [[56.669807  ]\n",
            "  [56.669807  ]\n",
            "  [56.669807  ]\n",
            "  ...\n",
            "  [ 2.0499554 ]\n",
            "  [ 2.0499554 ]\n",
            "  [ 2.0499554 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[25.992481  ]\n",
            "  [25.992481  ]\n",
            "  [25.992481  ]\n",
            "  ...\n",
            "  [-1.115045  ]\n",
            "  [-1.115045  ]\n",
            "  [-1.115045  ]]\n",
            "\n",
            " [[56.299694  ]\n",
            "  [56.299694  ]\n",
            "  [56.299694  ]\n",
            "  ...\n",
            "  [-1.0530806 ]\n",
            "  [-1.0530806 ]\n",
            "  [-1.0530806 ]]\n",
            "\n",
            " [[ 2.6239715 ]\n",
            "  [ 2.6239715 ]\n",
            "  [ 2.6239715 ]\n",
            "  ...\n",
            "  [ 0.51757807]\n",
            "  [ 0.51757807]\n",
            "  [ 0.51757807]]]\n",
            "\n",
            "Loaded Labels:\n",
            "[2 4 5 ... 2 0 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krshAu69Hy8-"
      },
      "source": [
        "Use train_test_split for splitting the train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LV83ruiHvfO"
      },
      "source": [
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(studio_recorded_features, studio_recorded_labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43M0H5Z23rnh"
      },
      "source": [
        "Load the dataset with DataLoader\n",
        "- Refer to [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)\n",
        "- Refer to [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6gI08XH2ak"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert training features and labels to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "# Create a TensorDataset for the training data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "# Create a DataLoader for the testing dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGq6XpvhFynP"
      },
      "source": [
        "### b) Define your CNN architecture (4 Marks)\n",
        "\n",
        "[Hint](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5hdERsFw5o"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Sample Convolution Layer 1\n",
        "        self.conv1 = nn.Conv1d(in_channels=900, out_channels=400, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm1d(400)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Sample Maxpool for the Convolutional Layer 1\n",
        "        self.maxpool1 = nn.MaxPool1d(1)\n",
        "\n",
        "        # Sample Dropout Layer\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "        # Additional Convolutional Layers\n",
        "        self.conv2 = nn.Conv1d(in_channels=400, out_channels=200, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm1d(200)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool1d(1)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=100, kernel_size=1)\n",
        "        self.bn3 = nn.BatchNorm1d(100)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool1d(1)\n",
        "\n",
        "        # Fully Connected Layer and LogSoftmax\n",
        "        self.fc = nn.Linear(100, 6)  # 6 output classes\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution Layer 1, Maxpool, and Dropout\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Convolution Layer 2, Maxpool, and Dropout\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.maxpool2(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Convolution Layer 3, Maxpool, and Dropout\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.maxpool3(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Flatten the output of the final pooling layer\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Apply LogSoftmax\n",
        "        out = self.logsoftmax(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OFWuGmq05ZK",
        "outputId": "4c892cb9-5a2b-4adc-9496-fd168af7f6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# To run the training on GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkt8lKQtCIWD",
        "outputId": "15509f78-3bdc-4084-a9fe-61ee31f4bcd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Net()\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss function (criterion)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv1d(900, 400, kernel_size=(1,), stride=(1,))\n",
            "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu1): ReLU()\n",
            "  (maxpool1): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (conv2): Conv1d(400, 200, kernel_size=(1,), stride=(1,))\n",
            "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu2): ReLU()\n",
            "  (maxpool2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv1d(200, 100, kernel_size=(1,), stride=(1,))\n",
            "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu3): ReLU()\n",
            "  (maxpool3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
            "  (logsoftmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nF5pwKQ2t1"
      },
      "source": [
        "### c) Train and classify on the studio_data (3 Marks)\n",
        "\n",
        "The goal here is to train the Model on voice samples collected in studio data and validate it continuously to calculate the loss and accuracy for the train dataset across each epoch.\n",
        "\n",
        "Iterate over images in the train_loader and perform the following steps.\n",
        "\n",
        "1. First, zero out the gradients using zero_grad()\n",
        "\n",
        "2. Pass the data to the model. Convert the data to GPU before passing data  to the model\n",
        "\n",
        "3. Calculate the loss using a Loss function\n",
        "\n",
        "4. Perform Backward pass using backward() to update the weights\n",
        "\n",
        "5. Optimize and predict by using the torch.max()\n",
        "\n",
        "6. Calculate the accuracy of the train dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ot89MxKavVy",
        "outputId": "2ceb857f-2598-47b5-df83-a43021b356ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Loop over the dataset for the specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize variables to track loss and accuracy\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Iterate over the training dataset\n",
        "    for data in train_loader:\n",
        "        # Get the inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Move inputs and labels to the appropriate device (GPU or CPU)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        # Predict\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Update accuracy statistics\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss and accuracy for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.3838, Accuracy: 47.03%\n",
            "Epoch [2/10], Loss: 0.9056, Accuracy: 68.71%\n",
            "Epoch [3/10], Loss: 0.7173, Accuracy: 75.21%\n",
            "Epoch [4/10], Loss: 0.6507, Accuracy: 76.88%\n",
            "Epoch [5/10], Loss: 0.5681, Accuracy: 80.43%\n",
            "Epoch [6/10], Loss: 0.5363, Accuracy: 80.49%\n",
            "Epoch [7/10], Loss: 0.5012, Accuracy: 82.12%\n",
            "Epoch [8/10], Loss: 0.4528, Accuracy: 83.41%\n",
            "Epoch [9/10], Loss: 0.4374, Accuracy: 85.08%\n",
            "Epoch [10/10], Loss: 0.4014, Accuracy: 86.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5BJIQzgHa0k"
      },
      "source": [
        "### d) Testing Evaluation for CNN model (3 Marks)\n",
        "\n",
        "Evaluate model with the given test data\n",
        "\n",
        "1. Transform and load the test images.\n",
        "\n",
        "2. Pass the test data through the model (network) to get the outputs\n",
        "\n",
        "3. Get the predictions from a maximum value using torch.max\n",
        "\n",
        "4. Compare with the actual labels and get the count of the correct labels\n",
        "\n",
        "5. Calculate the accuracy based on the count of correct labels\n",
        "\n",
        "### **Expected testing accuracy is above 80%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnKZ-gP-30xR",
        "outputId": "9d45b517-e350-453c-9339-1f8ff8e77693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to track accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over the test DataLoader\n",
        "for data in test_loader:\n",
        "    # Get the inputs and labels\n",
        "    inputs, labels = data\n",
        "\n",
        "    # Move inputs and labels to the appropriate device (GPU or CPU)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Pass the inputs through the model to get the outputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Get the predictions from the maximum value using torch.max\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Compare with the actual labels and get the count of correct labels\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Update total count of labels\n",
        "    total += labels.size(0)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Testing Accuracy: {accuracy:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 85.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqIP3Y17byDq"
      },
      "source": [
        "### e) Save and download your model (2 Marks)\n",
        "\n",
        "**Save your model trained on studio data**\n",
        "\n",
        "* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n",
        "integrating model to the web application\n",
        "\n",
        " [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7KAIpLsI4Uj"
      },
      "source": [
        "# Define the file path where you want to save the model\n",
        "model_path = 'studio_model.pth'\n",
        "\n",
        "# Save the state dictionary of the model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print(f'Model saved to {model_path}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsCHKXubHAJB"
      },
      "source": [
        "Download your trained model using the code below\n",
        "* Give the path of model file to download through the browser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDmWXfPaHJZG"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('studio_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl3Ins3vSTQx"
      },
      "source": [
        "### f) Deploy and evaluate your model trained on Studio Data in the server (6 Marks).\n",
        "\n",
        "(This can be done on the day of the Hackathon once the login username and password provided by the mentors in the lab)\n",
        "\n",
        "Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based e-commerce ordering.pdf) for details.\n",
        "\n",
        "To order product in user interface, go through the document (3-Hackathon_II Application Interface Documentation.pdf) for details.\n",
        "\n",
        "\n",
        "**Evaluation Criteria: Four consecutive utterances should be predicted correctly by the model**\n",
        "\n",
        "- There are two stages in the e-commerce ordering application    \n",
        "    - Ordering Product\n",
        "    - Selecting the e-commerce platform\n",
        "- If both the stages are cleared as per the evaluation criteria you will get\n",
        "complete marks Otherwise, you will see a reduction in the marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIXBC0aYKhKX"
      },
      "source": [
        "## **Stage 2:** Collect your voice samples and refine the classifier trained on studio_data and Team_data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSoJN11_kMR"
      },
      "source": [
        "### a) Collect your Team Voice Samples and extract features (6 Marks)\n",
        "\n",
        "(This can be done on the day of the Hackathon once the login username and password is given by mentors in the lab)\n",
        "\n",
        "* In order to collect the team data, ensure the server is active (2-Server Access and File transfer For Voice based e-commerce ordering.pdf)\n",
        "\n",
        "* Refer document \"3-Hackathon_II Application Interface Documentation.pdf\" for collecting your team voice samples. These will get stored in your server\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "* Load 'Team_data' and extract features\n",
        "* Combine features of team data with the extracted features of studio data\n",
        "* Split the combined features into train and test data\n",
        "* Load the dataset with DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv3I24flWlLq"
      },
      "source": [
        "!mkdir team_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB_bSllKWJ5U"
      },
      "source": [
        "# Replace <YOUR_GROUP_ID> with your Username given in the lab\n",
        "!wget -r -A .wav https://aiml-sandbox1.talentsprint.com/audio_recorder/<YOUR_GROUP_ID>/team_data/ -nH --cut-dirs=100  -P ./team_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G17PFkgI02J"
      },
      "source": [
        "# YOUR CODE HERE to Load data from teamdata folder for extracting all features and labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_sh6DCbJKhg"
      },
      "source": [
        "# Combine the features of all voice samples (studio_data and teamdata)\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcSfoU9hyst4"
      },
      "source": [
        "# YOUR CODE HERE to split the combined features into train and test data (Hint: Use train_test_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ8gSTYg1dSp"
      },
      "source": [
        "# YOUR CODE HERE to load the dataset with DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSE9E9mDF7Az"
      },
      "source": [
        "### b) Classify and download the model (6 Marks)\n",
        "\n",
        "The goal here is to train and test your model on all voice samples collected in studio and team data\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "* Refine your classifier (if needed)\n",
        "* Train your model on the extracted train data\n",
        "* Test your model on the extracted test data\n",
        "* Save and download the trained model\n",
        "\n",
        "### **Expected testing accuracy is above 80%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1EtSEwDG-q4"
      },
      "source": [
        "# YOUR CODE HERE for refining your classifier (if needed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz0UbGJrz59Z"
      },
      "source": [
        "# YOUR CODE HERE to train your model\n",
        "\n",
        "# Record loss and accuracy of the train dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CkqFppJ0Fha"
      },
      "source": [
        "# YOUR CODE HERE to test your model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snSilDehQcKv"
      },
      "source": [
        "**Save your trained model**\n",
        "\n",
        "* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n",
        "integrating model to the web application\n",
        "\n",
        " [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGiaYmCnQcKz"
      },
      "source": [
        "### YOUR CODE HERE for saving the CNN model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TSFyHRFQcK1"
      },
      "source": [
        "Download your trained model using the code below\n",
        "* Give the path of model file to download through the browser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF2kGMjAQcK2"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('<model_file_path>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfhmbXkFx6is"
      },
      "source": [
        "### c) Deploy and evaluate your model trained on Studio Data + Team Data in the server (6 Marks).\n",
        "\n",
        "(This can be done on the day of the Hackathon once the login username and password provided by the mentors in the lab)\n",
        "\n",
        "Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based e-commerce ordering.pdf) for details.\n",
        "\n",
        "To order product in user interface, go through the document (3-Hackathon_II Application Interface Documentation.pdf) for details.\n",
        "\n",
        "\n",
        "**Evaluation Criteria: Four consecutive utterances should be predicted correctly by the model**\n",
        "\n",
        "- There are two stages in the e-commerce ordering application    \n",
        "    - Ordering Product\n",
        "    - Selecting the e-commerce platform\n",
        "- If both the stages are cleared as per the evaluation criteria you will get\n",
        "complete marks Otherwise, you will see a reduction in the marks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kIo3R5hQnrh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}